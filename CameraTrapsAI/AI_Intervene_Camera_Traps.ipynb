{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b19ed325",
      "metadata": {
        "id": "b19ed325"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbsantiago/AI-Intervene-Training-Material/blob/main/CameraTrapsAI/AI_Intervene_Camera_Traps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85663c8-e4a4-4439-a3bb-55a9796c3f20",
      "metadata": {
        "id": "b85663c8-e4a4-4439-a3bb-55a9796c3f20"
      },
      "source": [
        "# AI for Wildlife Images\n",
        "\n",
        "In this notebook, you'll explore how AI can be used for large-scale ecological research. We'll use a real-world case study: a camera trap project conducted by a UCL team in Kenya's Maasai Mara ecosystem.\n",
        "\n",
        "Hereâ€™s a brief overview of what you will cover:\n",
        "\n",
        "* Camera Traps: What they are and how they're used in ecology.\n",
        "* Image Annotation: The process of labelling camera trap images.\n",
        "* MegaDetector: An AI tool for automatically detecting animals in photos.\n",
        "* SpeciesNet: An AI tool for automatically identifying the species of those animals.\n",
        "* Model Evaluation: How to measure the performance of these AI tools.\n",
        "\n",
        "**Note:** Throughout this notebook, you'll find several \"Exercises\".\n",
        "These are prompts for reflection, designed to get you thinking critically about the concepts.\n",
        "You don't need to write down any formal answers, but we strongly encourage you to discuss your thoughts with your peers.\n",
        "The notebook is intended to raise more questions than it answers.\n",
        "\n",
        "**Note:** You will see cells with Python code throughout this notebook.\n",
        "Your task is simply to run them to produce the results we'll be discussing.\n",
        "There is no need to read or understand the code, but if you are interested and want to learn more, the tutors are happy to explain it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CFKQTkKBEZy2",
      "metadata": {
        "id": "CFKQTkKBEZy2"
      },
      "source": [
        "# Setup\n",
        "\n",
        "If this is your first time in Google Colab and feel disoriented, take a few minutes to go through this [intro guide](https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/01_Work_environment/01a_Intro_to_colab.ipynb).\n",
        "\n",
        "Before starting, you need to connect the lab's dataset to this notebook.\n",
        "Follow these three steps to get everything linked up."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc48042d-3543-4f66-8927-8c3d22621405",
      "metadata": {
        "id": "dc48042d-3543-4f66-8927-8c3d22621405",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## 1. Add a Shortcut to the Data\n",
        "\n",
        "The data needed for this notebook is in a public [Google Drive folder](https://drive.google.com/drive/folders/1W_k4zbADhx9rwxGQLm97KZUfXra9g3C9?usp=drive_link).\n",
        "You need to add a shortcut to this folder in your own Drive.\n",
        "\n",
        "**Note:** If you've already added this shortcut, you can skip this step and move on to mounting your Drive.\n",
        "\n",
        "First, open the link. Then, click the folder's name at the top of the page and select the \"Add shortcut\" option.\n",
        "\n",
        "**Important:** Be sure to click on the folder name on the top of the page and not on the individual file in the table below.\n",
        "\n",
        "<img src=\"https://github.com/mbsantiago/AI-Intervene-Training-Material/blob/main/CameraTrapsAI/assets/google_drive_shortcut.png?raw=1\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "A new window will pop up. Go to the \"All locations\" tab and choose \"My Drive\".\n",
        "\n",
        "<img src=\"https://github.com/mbsantiago/AI-Intervene-Training-Material/blob/main/CameraTrapsAI/assets/google_drive_all_locations.png?raw=1\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "The data folder should now appear in your 'drive' folder (check the 'Files' tab on the left)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a87882",
      "metadata": {
        "id": "37a87882"
      },
      "source": [
        "## 2. Mount Google Drive\n",
        "\n",
        "Run the code cell below. This gives the notebook permission to access your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1zg2j5Nm3Fq5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zg2j5Nm3Fq5",
        "outputId": "c20eff6f-779a-41f4-9422-e5a758c96401",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GFtTDI2XzrFv",
      "metadata": {
        "id": "GFtTDI2XzrFv"
      },
      "source": [
        "## 3. Run the Setup script\n",
        "\n",
        "Run the cell below to complete the setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "421f0a0b-cb70-4a93-b500-7c7dc64676bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "421f0a0b-cb70-4a93-b500-7c7dc64676bf",
        "outputId": "ed8550a7-99ab-4f1c-bce1-44d6376ebc65",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading colab_utils.py\n",
            "Downloading ct_notebook_utils.py\n",
            "Using CPython 3.12.11 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.mdvenv/\u001b[39m\n",
            "Activate with: \u001b[32msource .mdvenv/bin/activate\u001b[39m\n",
            "\u001b[2mUsing Python 3.12.11 environment at: .mdvenv\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m Package(s) not found for: \u001b[1mmegadetector\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.11 environment at: .mdvenv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m111 packages\u001b[0m \u001b[2min 2.91s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m110 packages\u001b[0m \u001b[2min 1m 03s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m111 packages\u001b[0m \u001b[2min 969ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mabsl-py\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1malbucore\u001b[0m\u001b[2m==0.0.24\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==2.0.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.10.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclipboard\u001b[0m\u001b[2m==0.0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfire\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.75.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mintel-cmplr-lib-ur\u001b[0m\u001b[2m==2024.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mintel-openmp\u001b[0m\u001b[2m==2024.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonpickle\u001b[0m\u001b[2m==4.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmegadetector\u001b[0m\u001b[2m==10.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmkl\u001b[0m\u001b[2m==2024.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpycocotools\u001b[0m\u001b[2m==2.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyperclip\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyqtree\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytest\u001b[0m\u001b[2m==8.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msend2trash\u001b[0m\u001b[2m==1.8.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msimsimd\u001b[0m\u001b[2m==6.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstringzilla\u001b[0m\u001b[2m==4.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtbb\u001b[0m\u001b[2m==2021.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorboard\u001b[0m\u001b[2m==2.20.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorboard-data-server\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtermcolor\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mthop\u001b[0m\u001b[2m==0.1.1.post2209072238\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1multralytics-yolov5\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myolov9pip\u001b[0m\u001b[2m==0.0.4\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.11 environment at: .mdvenv\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m Package(s) not found for: \u001b[1mspeciesnet\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.11 environment at: .mdvenv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m107 packages\u001b[0m \u001b[2min 1.33s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m28 packages\u001b[0m \u001b[2min 2.74s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m28 packages\u001b[0m \u001b[2min 307ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.40.47\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.40.47\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcloudpathlib\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.35.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkagglehub\u001b[0m\u001b[2m==0.3.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mml-dtypes\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnx\u001b[0m\u001b[2m==1.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnx2torch\u001b[0m\u001b[2m==1.5.15\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.10.0.84\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpi-heif\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow-avif-plugin\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.32.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybboxes\u001b[0m\u001b[2m==0.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreverse-geocoder\u001b[0m\u001b[2m==1.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mroboflow\u001b[0m\u001b[2m==1.2.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msahi\u001b[0m\u001b[2m==0.11.36\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshapely\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mspeciesnet\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mterminaltables\u001b[0m\u001b[2m==3.1.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myolov5\u001b[0m\u001b[2m==7.0.11\u001b[0m\n",
            "Archive:  drive/MyDrive/AI-Intervene-Camera-Traps/data.zip\n",
            "   creating: ./data/\n",
            "   creating: ./data/.ipynb_checkpoints/\n",
            "  inflating: ./data/cameras.csv      \n",
            "  inflating: ./data/manual_labels.csv  \n",
            "   creating: ./data/metadata/\n",
            "  inflating: ./data/metadata/images.parquet  \n",
            "   creating: ./data/gis/\n",
            "   creating: ./data/gis/StudyArea/\n",
            "  inflating: ./data/gis/StudyArea/Study_area.sbn  \n",
            "  inflating: ./data/gis/StudyArea/Study_area.prj  \n",
            "  inflating: ./data/gis/StudyArea/Study_area.dbf  \n",
            "  inflating: ./data/gis/StudyArea/Study_area.shp  \n",
            "  inflating: ./data/gis/StudyArea/Study_area.shx  \n",
            " extracting: ./data/gis/StudyArea/Study_area.cpg  \n",
            "  inflating: ./data/gis/StudyArea/Study_area.sbx  \n",
            "   creating: ./data/gis/MaraNorthConservancy/\n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.shp.xml  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.prj  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.sbx  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.sbn  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.dbf  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.shp  \n",
            " extracting: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.cpg  \n",
            "  inflating: ./data/gis/MaraNorthConservancy/Mara_North_Conservancy.shx  \n",
            "   creating: ./data/gis/OlareOrokConservancy/\n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.dbf  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.prj  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.shp.xml  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.sbn  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.shp  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.shx  \n",
            "  inflating: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.sbx  \n",
            " extracting: ./data/gis/OlareOrokConservancy/Olare_Orok_Conservancy.cpg  \n",
            "   creating: ./data/gis/NaboishoConservancy/\n",
            " extracting: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.cpg  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.prj  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.sbx  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.shp.xml  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.shp  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.sbn  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.dbf  \n",
            "  inflating: ./data/gis/NaboishoConservancy/Naboisho_Conservancy.shx  \n",
            "   creating: ./data/gis/Triangle/\n",
            " extracting: ./data/gis/Triangle/Triangle.shp.MCP.10728.12472.sr.lock  \n",
            "  inflating: ./data/gis/Triangle/Triangle.shp.xml  \n",
            "  inflating: ./data/gis/Triangle/Triangle.dbf  \n",
            " extracting: ./data/gis/Triangle/Triangle.cpg  \n",
            "  inflating: ./data/gis/Triangle/Triangle.shx  \n",
            "  inflating: ./data/gis/Triangle/Triangle.prj  \n",
            "  inflating: ./data/gis/Triangle/Triangle.shp  \n",
            "   creating: ./data/gis/MotorogiConservancy/\n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.dbf  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.sbx  \n",
            " extracting: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.cpg  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.shp.xml  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.prj  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.sbn  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.shx  \n",
            "  inflating: ./data/gis/MotorogiConservancy/Motorogi_Conservancy.shp  \n",
            "  inflating: ./data/manual_labels.parquet  \n",
            "   creating: ./data/images/\n",
            "  inflating: ./data/images/2018_MT22_020230.JPG  \n",
            "  inflating: ./data/images/2018_NB49_003774.JPG  \n",
            "  inflating: ./data/images/2018_NB40_002921.JPG  \n",
            "  inflating: ./data/images/2018_OMC29_006244.JPG  \n",
            "  inflating: ./data/images/2018_NB01_001794.JPG  \n",
            "  inflating: ./data/images/2018_MT42_003176.JPG  \n",
            "  inflating: ./data/images/2018_OMC04_001358.JPG  \n",
            "  inflating: ./data/images/2018_MN14_007864.JPG  \n",
            "  inflating: ./data/images/2018_MT27_005639.JPG  \n",
            "  inflating: ./data/images/2018_MT40_009674.JPG  \n",
            "  inflating: ./data/images/2018_MT24_028924.JPG  \n",
            "  inflating: ./data/images/2018_MT18_021939.JPG  \n",
            "  inflating: ./data/images/2018_NB47_008365.JPG  \n",
            "  inflating: ./data/images/2018_MN46_001415.JPG  \n",
            "  inflating: ./data/images/2018_MN45_004582.JPG  \n",
            "  inflating: ./data/images/2018_NB47_020734.JPG  \n",
            "  inflating: ./data/images/2018_NB41_004031.JPG  \n",
            "  inflating: ./data/images/2018_MT17_021217.JPG  \n",
            "  inflating: ./data/images/2018_OMC11_009862.JPG  \n",
            "  inflating: ./data/images/2018_MT41_004920.JPG  \n",
            "  inflating: ./data/images/2018_MT13_021307.JPG  \n",
            "  inflating: ./data/images/2018_MN46_001424.JPG  \n",
            "  inflating: ./data/images/2018_NB23_001896.JPG  \n",
            "  inflating: ./data/images/2018_NB26_000679.JPG  \n",
            "  inflating: ./data/images/2018_NB26_025049.JPG  \n",
            "  inflating: ./data/images/2018_NB47_006126.JPG  \n",
            "  inflating: ./data/images/2018_MN48_007284.JPG  \n",
            "  inflating: ./data/images/2018_NB42_021230.JPG  \n",
            "  inflating: ./data/images/2018_OMC01_021306.JPG  \n",
            "  inflating: ./data/images/2018_OMC10_024437.JPG  \n",
            "  inflating: ./data/images/2018_NB20_001567.JPG  \n",
            "  inflating: ./data/images/2018_NB47_010910.JPG  \n",
            "  inflating: ./data/images/2018_MN15_009733.JPG  \n",
            "  inflating: ./data/images/2018_NB11_002161.JPG  \n",
            "  inflating: ./data/images/2018_MN48_027239.JPG  \n",
            "  inflating: ./data/images/2018_NB47_006890.JPG  \n",
            "  inflating: ./data/images/2018_NB05_002216.JPG  \n",
            "  inflating: ./data/images/2018_OMC34_021477.JPG  \n",
            "  inflating: ./data/images/2018_MN33_009632.JPG  \n",
            "  inflating: ./data/images/2018_NB11_002776.JPG  \n",
            "  inflating: ./data/images/2018_MT40_026283.JPG  \n",
            "  inflating: ./data/images/2018_MT03_014932.JPG  \n",
            "  inflating: ./data/images/2018_OMC26_004593.JPG  \n",
            "  inflating: ./data/images/2018_NB47_001624.JPG  \n",
            "  inflating: ./data/images/2018_NB09_005484.JPG  \n",
            "  inflating: ./data/images/2018_NB04_022546.JPG  \n",
            "  inflating: ./data/images/2018_MT37_005128.JPG  \n",
            "  inflating: ./data/images/2018_NB03_007591.JPG  \n",
            "  inflating: ./data/images/2018_MN25_021497.JPG  \n",
            "  inflating: ./data/images/2018_NB03_001071.jpg  \n",
            "   creating: ./data/images/.ipynb_checkpoints/\n",
            "  inflating: ./data/images/.ipynb_checkpoints/2018_OMC34_021477-checkpoint.JPG  \n",
            "  inflating: ./data/images/.ipynb_checkpoints/2018_NB49_003774-checkpoint.JPG  \n",
            "   creating: ./data/metadata/.ipynb_checkpoints/\n",
            "  inflating: ./data/metadata/.ipynb_checkpoints/labels-checkpoint.csv  \n",
            "  inflating: ./data/metadata/cameras.csv  \n",
            "  inflating: ./data/metadata/labels.csv  \n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "!wget -q -O - https://github.com/mbsantiago/AI-Intervene-Training-Material/raw/refs/heads/main/CameraTrapsAI/ct_notebook_setup.sh | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ae1e94-4177-4b19-a646-675fc7deb980",
      "metadata": {
        "id": "67ae1e94-4177-4b19-a646-675fc7deb980"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Camera traps** are static cameras set up in the wild to monitor animal populations.\n",
        "Typically, they are triggered by motion, capturing an image whenever an animal moves within the camera's field of view.\n",
        "\n",
        "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/bios0032_23-24/3_AI_for_Wildlife_Images/images/ct.png?raw=true\" alt=\"drawing\" width=\"200\"/>\n",
        "\n",
        "The images from camera traps show which species are in an area and what they're doing.\n",
        "This data reveals how often animals appear, their activity times, and their general behaviour.\n",
        "Camera traps are also relatively easy to set up and are great for capturing a wide range of medium to large animals.\n",
        "\n",
        "By combining this animal data with environmental information, ecologists can answer important questions.\n",
        "For example, a key goal is to understand how wildlife reacts to human pressure.\n",
        "By comparing animal communities in areas with low versus high human impact, it's possible to find thresholds, or tipping points, where the ecosystem changes significantly.\n",
        "Finding these thresholds helps inform conservation decisions, like where to create a protected area or what land-use rules to set.\n",
        "Placing cameras strategically along these areas of varying human impact provides the data needed to answer these kinds of questions.\n",
        "\n",
        "After the cameras are collected, the biggest challenge is going through all the photos.\n",
        "A huge number of these images are \"false triggers\" with no animals, set off by things like waving grass.\n",
        "Even when an animal is in the photo, it can be hard to identify if the view is bad, it's too far away, or it's an unfamiliar species.\n",
        "On top of that, a single project can produce hundreds of thousands or even millions of images.\n",
        "Manually checking every photo is extremely slow and often impossible for large studies.\n",
        "\n",
        "This is one of the key ways AI is changing ecological work.\n",
        "It helps by automating the slow task of sorting photos, which in turn makes large-scale studies using camera traps possible.\n",
        "In this notebook, you will work with an example dataset from the Biome Health Project, learning how to use AI to process and analyse the collected imagery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75fdc32-49e0-41f4-b8ed-8bdc19804bf9",
      "metadata": {
        "id": "a75fdc32-49e0-41f4-b8ed-8bdc19804bf9"
      },
      "source": [
        "# Biome Health Project\n",
        "\n",
        "The dataset you'll use comes from the Biome Health Project.\n",
        "A key goal of this project is to study how wildlife responds to different levels of human pressure.\n",
        "By understanding this response in detail, the project aims to identify specific pressure thresholds that can be used to make conservation actions, like setting land-use limits, more effective.\n",
        "\n",
        "![Responses](https://static.wixstatic.com/media/d56724_6d6b60fecd174d24a714672dafcf00cf~mv2.png/v1/crop/x_8,y_0,w_829,h_588/fill/w_829,h_588,al_c,q_90,enc_avif,quality_auto/Gradient_3.png)\n",
        "\n",
        "This notebook focuses on data from the Greater Maasai Mara Ecosystem in Kenya, a savanna famous for its abundant wildlife.\n",
        "Below you see consecutive frames captured by a camera trap in Kenya.\n",
        "It shows a hyena entering the scene and checking out a buffalo!\n",
        "\n",
        "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/bios0032_23-24/3_AI_for_Wildlife_Images/images/hyena.gif?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "The study area contains a mix of different zones with different management rules.\n",
        "It includes a highly protected part of the Maasai Mara National Reserve alongside community-run conservancies.\n",
        "In the National Reserve, protection is very strict and the area is actively patrolled.\n",
        "In the community conservancies, Maasai landowners in partnership with tourism companies and follow specific rules for grazing their cattle.\n",
        "This setup creates a clear gradient of human and livestock pressure, from highly protected land to areas with more grazing and human presence.\n",
        "\n",
        "To monitor wildlife, a team from UCL and local conservancy staff placed camera traps across the landscape.\n",
        "A systematic approach was used to ensure the entire area was sampled evenly.\n",
        "Over 180 motion-activated cameras were set up across a huge 1,200 kmÂ² area in a 2x2 km grid pattern.\n",
        "In the centre of each grid square, one camera was mounted on a tree or post about 50 cm off the ground."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed8d329-ecbb-4bde-ba50-d8ea10b37280",
      "metadata": {
        "id": "8ed8d329-ecbb-4bde-ba50-d8ea10b37280"
      },
      "source": [
        "## **Exercise 1**: Explore the landscape\n",
        "\n",
        "Let's start by getting a feel for the landscape where the data was collected.\n",
        "\n",
        "Run the cell below to generate an interactive map of the study area. Each point on the map marks the location of a camera trap.\n",
        "\n",
        "Take a moment to explore the map.\n",
        "Zoom in, zoom out, and pan around the region.\n",
        "Use the layer selection tool on the top right to switch between different views, like satellite imagery, topography, and street maps.\n",
        "\n",
        "As you look, think about these questions:\n",
        "\n",
        "1. What major geographical features can you see? Look for things like rivers, hills, and potential changes in vegetation.\n",
        "2. How might these features affect which animals live there? For example, would a leopard prefer a rocky outcrop or an open plain?\n",
        "3. To build a better mental picture, search online for images of the \"Maasai Mara National Reserve\". How does the environment look on the ground?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06244aac-5fdc-47ac-9bcf-98d4ec47b9fa",
      "metadata": {
        "cellView": "form",
        "id": "06244aac-5fdc-47ac-9bcf-98d4ec47b9fa"
      },
      "outputs": [],
      "source": [
        "# @title \"Camera Trap Location Map\"\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import folium\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "cameras = pd.read_csv(\"data/metadata/cameras.csv\")\n",
        "\n",
        "areas = {\n",
        "    \"Mara Triangle\": {\n",
        "        \"path\": \"data/gis/Triangle/\",\n",
        "        \"color\": \"green\",\n",
        "    },\n",
        "    \"Mara North Conservancy\": {\n",
        "        \"path\": \"data/gis/MaraNorthConservancy/\",\n",
        "        \"color\": \"blue\",\n",
        "    },\n",
        "    \"Motorogi Conservancy\": {\n",
        "        \"path\": \"data/gis/MotorogiConservancy/\",\n",
        "        \"color\": \"orange\",\n",
        "    },\n",
        "    \"Olare Orok Conservancy\": {\n",
        "        \"path\": \"data/gis/OlareOrokConservancy/\",\n",
        "        \"color\": \"orange\",\n",
        "    },\n",
        "    \"Naboisho Conservancy\": {\n",
        "        \"path\": \"data/gis/NaboishoConservancy/\",\n",
        "        \"color\": \"purple\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def style_fn(feature, color):\n",
        "    return {\"fillColor\": color, \"color\": color}\n",
        "\n",
        "\n",
        "def get_regime(location_id):\n",
        "    conservancy = location_id[:-2]\n",
        "    return {\n",
        "        \"MT\": \"Mara Triangle\",\n",
        "        \"MN\": \"Mara North Conservancy\",\n",
        "        \"OMC\": \"Motorogi Conservancy\",\n",
        "        \"NB\": \"Naboisho Conservancy\",\n",
        "    }[conservancy]\n",
        "\n",
        "\n",
        "m = folium.Map(tiles=None)\n",
        "\n",
        "folium.TileLayer(\"OpenTopoMap\", overlay=False).add_to(m)\n",
        "folium.TileLayer(\"Esri.WorldImagery\", overlay=False).add_to(m)\n",
        "folium.TileLayer(\"OpenStreetMap\", overlay=False).add_to(m)\n",
        "\n",
        "crs = \"EPSG:4326\"\n",
        "\n",
        "m.fit_bounds(\n",
        "    [\n",
        "        [cameras.Latitude.min(), cameras.Longitude.min()],\n",
        "        [cameras.Latitude.max(), cameras.Longitude.max()],\n",
        "    ]\n",
        ")\n",
        "\n",
        "for _, row in cameras.iterrows():\n",
        "    regime = get_regime(row[\"Location ID\"])\n",
        "    color = areas[regime][\"color\"]\n",
        "    folium.Marker(\n",
        "        location=[row.Latitude, row.Longitude],\n",
        "        icon=folium.Icon(prefix=\"fa\", icon=\"camera\", color=color),\n",
        "        popup=f\"Location ID = {row['Location ID']}\",\n",
        "    ).add_to(m)\n",
        "\n",
        "for name, data in areas.items():\n",
        "    area = gpd.read_file(data[\"path\"]).to_crs(crs)\n",
        "    layer = folium.GeoJson(\n",
        "        area.to_json(),\n",
        "        style_function=partial(style_fn, color=data[\"color\"]),\n",
        "        name=name,\n",
        "    )\n",
        "    folium.Popup(name).add_to(layer)\n",
        "    layer.add_to(m)\n",
        "\n",
        "folium.LayerControl().add_to(m)\n",
        "\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b565a39f-0066-4d0f-8a4e-f91afdd7d13a",
      "metadata": {
        "id": "b565a39f-0066-4d0f-8a4e-f91afdd7d13a"
      },
      "source": [
        "# Data Collection\n",
        "\n",
        "The dataset for this lab comes from a single year, 2018.\n",
        "For this year alone we collected a total of 2.4 million images.\n",
        "\n",
        "At this stage, we have no idea what's in these photos.\n",
        "They could be rare animals or just empty shots triggered by moving branches.\n",
        "A good first step in any camera trap project is to analyse the metadata (the information about the images), like when and where they were taken.\n",
        "This helps us understand the data collection process itself."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81df4adf-ecba-4e03-8dd3-6354c14b41da",
      "metadata": {
        "id": "81df4adf-ecba-4e03-8dd3-6354c14b41da"
      },
      "source": [
        "## **Exercise 2**: Visualise the Collection Effort\n",
        "\n",
        "Let's visualise the entire 2018 data collection effort.\n",
        "Run the cell below to generate a plot that gives an overview of when and where photos were taken.\n",
        "\n",
        "Each column represents a single camera trap site.\n",
        "Each row is a day of the year (from 1 to 365, but we focus on 260-340).\n",
        "The color of each pixel shows the number of images taken at that site on that day. White means no images were captured.\n",
        "\n",
        "Notice that the site names tell you which area they belong to (e.g., the Mara Triangle or a specific conservancy).\n",
        "\n",
        "Now, take a close look at the plot and think about these questions:\n",
        "\n",
        "1. Why are there gaps and \"noise\" in the plot?\n",
        "2. Do you see any broad patterns between the different areas? For example, did data collection start and stop at the same time everywhere?\n",
        "3. Look at the color bar to see the range of values. Some sites have days with tens of thousands of images. Does a high image count for a site directly translate to high animal abundance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f560cb73-34c6-4849-a206-4f4bdd39d164",
      "metadata": {
        "cellView": "form",
        "id": "f560cb73-34c6-4849-a206-4f4bdd39d164"
      },
      "outputs": [],
      "source": [
        "# @title \"Number of Images per Day\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
        "\n",
        "images_per_day = (\n",
        "    images_metadata.groupby(\n",
        "        [\n",
        "            images_metadata.site_id,\n",
        "            images_metadata.datetime.dt.day_of_year.rename(\"day_of_year\"),\n",
        "        ],\n",
        "        observed=True,\n",
        "    )\n",
        "    .size()\n",
        "    .rename(\"num_images\")\n",
        "    .reset_index()\n",
        "    .pivot(index=\"site_id\", columns=\"day_of_year\", values=\"num_images\")\n",
        "    .fillna(0)\n",
        "    .T\n",
        ")\n",
        "\n",
        "images_per_day = images_per_day[images_per_day.index >= 260]\n",
        "\n",
        "_, ax = plt.subplots(figsize=(20, 8))\n",
        "\n",
        "ax.xaxis.tick_top()\n",
        "sns.heatmap(\n",
        "    images_per_day,\n",
        "    ax=ax,\n",
        "    mask=images_per_day == 0,\n",
        "    cmap=\"flare\",\n",
        ")\n",
        "ax.set(xlabel=\"Site ID\", ylabel=\"Day of Year\")\n",
        "\n",
        "ax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752bce17-caea-4499-9aa3-ed3a787fa455",
      "metadata": {
        "id": "752bce17-caea-4499-9aa3-ed3a787fa455"
      },
      "source": [
        "# Manual Annotation\n",
        "\n",
        "So, how do you find the animals in 2.4 million photos? Before jumping into AI solutions, it's helpful to understand the traditional, manual approach.\n",
        "\n",
        "The process of reviewing images to record information about them, like which species are present, is called **annotation** or **labelling**.\n",
        "\n",
        "Before AI became common, all camera trap research relied on people, researchers, experts, or citizen scientists, manually annotating every single photo.\n",
        "While this is incredibly slow, it has a major benefit: it gives the annotator a much closer familiarity with the data.\n",
        "By looking through thousands of images, you gain an intuitive sense of what the data looks like, what to expect, and where potential issues might arise.\n",
        "This hands-on experience is invaluable for correctly interpreting the final results of any analysis.\n",
        "\n",
        "Manual annotation is also essential for building AI models.\n",
        "The human-labelled images serve as the \"ground truth\" used to both train an AI model and test how well it performs.\n",
        "We'll cover that in more detail later.\n",
        "For now, it's time to get a feel for the annotation process yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O_MWcffUftFd",
      "metadata": {
        "id": "O_MWcffUftFd"
      },
      "source": [
        "## **Exercise 3**: Manual Annotation\n",
        "\n",
        "Your task is to review a small set of 10 images and detect any animals you see.\n",
        "\n",
        "Run the cell below to launch the annotation tool.\n",
        "\n",
        "Here are your instructions:\n",
        "\n",
        "* Draw a box around every animal you can find. If there are multiple animals in one photo, make sure to box each one.\n",
        "* Just detect, don't identify. For this exercise, your only task is to find the animals, not to name their species.\n",
        "* Try to make your boxes as tight as possible around the animal, without including too much background.\n",
        "* Remember, some images will be empty. If you don't see any animals, just move on to the next one.\n",
        "* Be thorough! Do your best to find every animal, even if it's small, far away, or partially hidden.\n",
        "\n",
        "When you've finished all 10 images, click the Submit button to save your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3aa7e67-6609-4abb-b9f5-f1bbba03f497",
      "metadata": {
        "id": "d3aa7e67-6609-4abb-b9f5-f1bbba03f497",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \"Annotator\"\n",
        "from pathlib import Path\n",
        "\n",
        "from ct_notebook_utils import Annotator\n",
        "\n",
        "# path for detection data\n",
        "data_dir = Path.cwd() / \"data\"\n",
        "\n",
        "image_dir = data_dir / \"images\"\n",
        "\n",
        "selected_images = [\n",
        "    Path(\"data/images/2018_NB01_001794.JPG\"),\n",
        "    Path(\"data/images/2018_NB40_002921.JPG\"),\n",
        "    Path(\"data/images/2018_MT22_020230.JPG\"),\n",
        "    Path(\"data/images/2018_OMC11_009862.JPG\"),\n",
        "    Path(\"data/images/2018_NB26_025049.JPG\"),\n",
        "    Path(\"data/images/2018_MN33_009632.JPG\"),\n",
        "    Path(\"data/images/2018_NB26_000679.JPG\"),\n",
        "    Path(\"data/images/2018_MT27_005639.JPG\"),\n",
        "    Path(\"data/images/2018_NB05_002216.JPG\"),\n",
        "    Path(\"data/images/2018_NB47_006890.JPG\"),\n",
        "]\n",
        "\n",
        "# create a list with the numpy arrays that correspond to each of the\n",
        "# images to annotate\n",
        "annotator = Annotator(selected_images)\n",
        "\n",
        "annotator.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9cfec9c-55c8-4026-8f36-a2c7a543c069",
      "metadata": {
        "id": "f9cfec9c-55c8-4026-8f36-a2c7a543c069"
      },
      "source": [
        "## **Exercise 4**: Annotation Costs\n",
        "\n",
        "That short annotation task gives you a feel for the process. But how does that effort scale up from 10 images to 2.4 million?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a995ee6e-4a9e-40a2-a8bb-1473b6537142",
      "metadata": {
        "id": "a995ee6e-4a9e-40a2-a8bb-1473b6537142"
      },
      "source": [
        "### **Step 1**: Get Your Annotation Stats\n",
        "\n",
        "Run the cell below to see a report on your work from the last exercise. It will show you how long it took to annotate the 10 images and how many animals you found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1jPaE4G1Ewyn",
      "metadata": {
        "id": "1jPaE4G1Ewyn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \"Annotation Report\"\n",
        "# run this cell straight after your annotation\n",
        "annotation_duration = annotator.duration\n",
        "tagged_image_boxes = annotator.annotations\n",
        "\n",
        "total_images = len(selected_images)\n",
        "\n",
        "# convert time to minutes\n",
        "print(f\"The annotation of the {total_images} images took {annotation_duration}\")\n",
        "\n",
        "# given produced annotation list, we calculate total animals and the number\n",
        "# of images with animals\n",
        "num_animals = sum(x.shape[0] for x in tagged_image_boxes if x is not None)\n",
        "num_non_empty_images = sum(x is not None for x in tagged_image_boxes)\n",
        "annotation_speed = annotation_duration / float(len(selected_images))\n",
        "print(\n",
        "    f\"In total, you found {num_animals} animals across {num_non_empty_images} images while \"\n",
        "    f\"{total_images - num_non_empty_images} out of the {total_images} images were tagged as empty.\\n\"\n",
        "    f\"You tagged 1 image every {annotation_speed}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6f3e2d-6005-459a-a319-25f84ceeac05",
      "metadata": {
        "id": "eb6f3e2d-6005-459a-a319-25f84ceeac05"
      },
      "source": [
        "### **Step 2:** Extrapolate to the Full Dataset\n",
        "\n",
        "Now, let's see what it would take to annotate the entire 2018 dataset. Run the next cell to launch an interactive tool that estimates the total time and cost.\n",
        "\n",
        "Play around with the sliders in the tool to see how the numbers change. You can control:\n",
        "\n",
        "* Number of annotators: How many people are working on the project?\n",
        "* Dataset percentage: Do you need to annotate all the images (100%), or just a smaller fraction?\n",
        "* Hourly pay rate ($): How much would you pay an annotator per hour?\n",
        "\n",
        "Think about the results. Is manually annotating the full 2.4 million images a feasible task for a typical research project?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c5b7cdb-75e0-44a7-8a15-0eb98bd976aa",
      "metadata": {
        "cellView": "form",
        "id": "5c5b7cdb-75e0-44a7-8a15-0eb98bd976aa"
      },
      "outputs": [],
      "source": [
        "# @title \"Extrapolating to the whole dataset\"\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "\n",
        "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
        "\n",
        "\n",
        "@widgets.interact(\n",
        "    num_annotators=(1, 20),\n",
        "    dataset_percentage=(0.0, 1.0, 0.05),\n",
        "    cost_per_hour=(6, 50),\n",
        ")\n",
        "def report_annotation_costs(num_annotators=1, dataset_percentage=1.0, cost_per_hour=17):\n",
        "    total_images = len(images_metadata)\n",
        "\n",
        "    num_images_to_annotate = int(total_images * dataset_percentage)\n",
        "\n",
        "    total_duration = annotation_speed * (num_images_to_annotate / num_annotators)\n",
        "    total_cost = total_duration.total_seconds() * cost_per_hour * num_annotators / 3600\n",
        "    print(\n",
        "        f\"At the same speed it would take {num_annotators} person(s) a total \"\n",
        "        f\"of {total_duration} to annotate {dataset_percentage:.1%} of the data ({num_images_to_annotate:,d} images).\\n\"\n",
        "        f\"This would cost {total_cost:,.2f}Â£ at an hourly rate of {cost_per_hour}Â£.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958fbf1d-1de2-4dae-a2dc-9db9f7875514",
      "metadata": {
        "id": "958fbf1d-1de2-4dae-a2dc-9db9f7875514"
      },
      "source": [
        "# MegaDetector\n",
        "\n",
        "Manually annotating millions of images is clearly not practical. This is where a pre-trained AI model can save a huge amount of time.\n",
        "\n",
        "For this notebook, we will use [**MegaDetector**](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md), a model initially developed by Microsoft's AI for Earth program.\n",
        "It was trained on millions of camera trap images from many different ecosystems, so it's generally robust and reliable.\n",
        "\n",
        "**Note:** Like any AI, MegaDetector learned to detect animals by looking at a huge library of training examples.\n",
        "It works best when your data is similar to what it was trained on.\n",
        "It might struggle with images from unique ecosystems or with different types of imagery (like from drones).\n",
        "No AI is perfect, and it will still make mistakes.\n",
        "You can see some examples of known failure cases [here](https://github.com/agentmorris/MegaDetector/blob/main/megadetector-challenges.md).\n",
        "\n",
        "Here are a few key things about it:\n",
        "\n",
        "* MegaDetector's purpose is to find and draw boxes around three types of objects: animals, people, and vehicles. It doesn't identify the species of the animal, it just finds it.\n",
        "\n",
        "* MegaDetector is \"pretrained\". This means it has already been trained on a large, general dataset. We can take this general, pre-trained model and apply it directly to our own images without any additional train.\n",
        "\n",
        "* For every box it draws, MegaDetector provides a confidence score between 0 and 1. A score close to 1.0 means the model is very certain about its detection (e.g., \"I'm 98% sure this is an animal\").\n",
        "\n",
        "* We can use these scores to automatically filter out empty images. By setting a confidence threshold, we can decide that any detection below a certain score isn't a \"real\" detection. Choosing this threshold is a trade-off: a lower threshold means you might find more animals but will also have to check more false positives."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZTbwGVXVoDdg",
      "metadata": {
        "id": "ZTbwGVXVoDdg"
      },
      "source": [
        "## **Exercise 5**: Run MegaDetector\n",
        "\n",
        "It's time to run MegaDetector on the same 10 images you annotated manually.\n",
        "This will give you a direct comparison between your work and the AI's predictions.\n",
        "\n",
        "Using an AI model isn't always simple.\n",
        "In general, you have to find the model's code repository (like on GitHub) and read the developers' notes to get it working.\n",
        "Most models are built with deep learning frameworks (like [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/)), and using them often requires some Python scripting.\n",
        "\n",
        "Fortunately, because MegaDetector is so popular, there are many ways to use it:\n",
        "\n",
        "* Desktop Apps: User-friendly tools like [Addax](https://addaxdatascience.com/addaxai/) provide a graphical interface.\n",
        "* Cloud Services: Platforms like [Wildlife Insights](https://wildlifeinsights.org/) have integrated MegaDetector into their workflow.\n",
        "* Code: You can always run it directly from a Python script, see its [official docs](https://megadetector.readthedocs.io/en/latest/).\n",
        "\n",
        "### Step 1: Run MegaDetector command\n",
        "\n",
        "For this exercise, we'll use another common method: the command line.\n",
        "This approach allows us to run the model by typing a single command with a few parameters, telling it where the images are and how to process them.\n",
        "It's a great way to run tools without needing a graphical interface or writing a full script.\n",
        "\n",
        "**Note:** If you're new to the command line, think of it as a text-based way to give instructions to your computer.\n",
        "You type commands into an application called a \"terminal\" to run programs or manage files.\n",
        "\n",
        "Run the cell below. It contains the command-line instruction to run MegaDetector on the example images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8zf1q6g-BzFg",
      "metadata": {
        "id": "8zf1q6g-BzFg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%%bash\n",
        "source .mdvenv/bin/activate\n",
        "python -m megadetector.detection.run_detector_batch MDV5A \\\n",
        "  \"data/images/\" \\\n",
        "  \"data/results/md_detections.json\" \\\n",
        "  --output_relative_filenames \\\n",
        "  --threshold 0.2 \\\n",
        "  --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8qEhxiwch1Er",
      "metadata": {
        "id": "8qEhxiwch1Er"
      },
      "source": [
        "### Step 2: Measure Speed\n",
        "\n",
        "Once the process is finished, the cell output will show some statistics about how fast the model ran.\n",
        "\n",
        "Find the line that looks like this:\n",
        "\n",
        "    Finished inference for N images in M minutes and x seconds (y images per second)\n",
        "\n",
        "Take note of the processing speed (the \"images per second\" value), as we'll use in the cell below.\n",
        "\n",
        "\n",
        "**Note:** The first time you run the model, it takes a bit longer because the model files have to be downloaded and prepared for processing.\n",
        "This is a one-time setup cost.\n",
        "To get a more accurate measure of the model's true processing speed, it's a good idea to run the cell a second time and use that value.\n",
        "\n",
        "Now that you have the processing speed, let's extrapolate. How long would it take MegaDetector to process the entire 2.4 million image dataset?\n",
        "\n",
        "Run the cell below to launch another interactive tool. Input your \"images per second\" value from the previous step. You can also use the slider to see how the total time changes if you only need to process a fraction of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a7d59f-e4e5-4777-8671-8c4c0c3e4525",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "21a7d59f-e4e5-4777-8671-8c4c0c3e4525"
      },
      "outputs": [],
      "source": [
        "# @title \"MegaDetector Speed\"\n",
        "import datetime\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "\n",
        "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
        "\n",
        "\n",
        "@widgets.interact(\n",
        "    images_per_sec=widgets.BoundedFloatText(\n",
        "        value=0.28,\n",
        "        min=0,\n",
        "        max=10.0,\n",
        "        step=0.01,\n",
        "        description=\"Images per Sec:\",\n",
        "        disabled=False,\n",
        "    ),\n",
        "    dataset_percentage=(0.0, 1.0, 0.05),\n",
        ")\n",
        "def compute_megadetector_costs(\n",
        "    images_per_sec=0.28,\n",
        "    dataset_percentage=1,\n",
        "):\n",
        "    duration = datetime.timedelta(seconds=1 / images_per_sec)\n",
        "    images_to_process = int(dataset_percentage * len(images_metadata))\n",
        "    total_duration = duration * images_to_process\n",
        "    print(\n",
        "        f\"MegaDetector would take {total_duration} to process {dataset_percentage:.0%} of the dataset (i.e. {images_to_process} images)\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98508ec-059a-498b-86b1-d36176ad9c3d",
      "metadata": {
        "id": "c98508ec-059a-498b-86b1-d36176ad9c3d"
      },
      "source": [
        "### Step 3: Think about Hardware\n",
        "\n",
        "It's important to know that the model's processing speed heavily depends on the computer's hardware.\n",
        "\n",
        "Most AI models run massively faster on a GPU (Graphics Processing Unit) compared to a standard CPU.\n",
        "A GPU is a specialised chip, most often used for rendering graphics, that is extremely good at the types of calculations AI requires.\n",
        "\n",
        "Not every computer has a compatible GPU, and they can be expensive.\n",
        "However, using one can accelerate the process by orders of magnitude, turning a task that takes weeks into one that takes only a day.\n",
        "If you're curious, you can try re-running the MegaDetector step with different hardware settings in this notebook.\n",
        "\n",
        "**Note:** Curious to see the difference?\n",
        "You can change the hardware for this Colab notebook.\n",
        "Go to the \"Runtime\" menu at the top, select \"Change runtime type\", and choose a different \"Hardware accelerator\" (like GPU or CPU).\n",
        "\n",
        "**Warning:** Changing the runtime will disconnect you and start a new session, so you'll have to run all the setup steps from the beginning of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad84ec50-c081-4e15-8954-74c4dee5643e",
      "metadata": {
        "id": "ad84ec50-c081-4e15-8954-74c4dee5643e"
      },
      "source": [
        "## **Exercise 6**: MegaDetector Outputs\n",
        "\n",
        "When MegaDetector finished, it saved all its findings into a single output file located at: `data/results/md_detections.json`.\n",
        "\n",
        "This is a JSON file, a common format for storing structured data.\n",
        "While scientists often work with tables (like CSV files or Excel spreadsheets), JSON is widely used in software and on the web.\n",
        "It's essentially a text file, but the information is organised in a specific, nested way that can look a bit confusing at first."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986ff5c3-70a8-4eb1-b478-3cf199abeeb7",
      "metadata": {
        "id": "986ff5c3-70a8-4eb1-b478-3cf199abeeb7"
      },
      "source": [
        "### Step 1: Take a Peek at the Raw JSON\n",
        "\n",
        "Let's get a feel for what this raw data format looks like.\n",
        "\n",
        "Use the file browser on the left to navigate to the `md_detections.json` file and double-click to open it.\n",
        "It will appear in a new tab.\n",
        "\n",
        "Don't worry about understanding every detail.\n",
        "The goal is just to see how the information is structured.\n",
        "When you're done, close the tab to come back to this notebook.\n",
        "\n",
        "Since JSON is such a common format, it's worth getting familiar with it.\n",
        "If you're curious to learn more, you can read about it [here](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aRCvjcnoFmXe",
      "metadata": {
        "id": "aRCvjcnoFmXe"
      },
      "source": [
        "### Step 2: Read the File with Code\n",
        "\n",
        "Now, let's use code to read that same JSON file and pull out a summary of what MegaDetector found. Did you get the same number of animals?\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fvWZRY6VMT7y",
      "metadata": {
        "id": "fvWZRY6VMT7y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \"MegaDetector Report\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "output_file_path = data_dir / \"results\" / \"md_detections.json\"\n",
        "\n",
        "selected_images = [\n",
        "    \"2018_NB01_001794.JPG\",\n",
        "    \"2018_NB40_002921.JPG\",\n",
        "    \"2018_MT22_020230.JPG\",\n",
        "    \"2018_OMC11_009862.JPG\",\n",
        "    \"2018_NB26_025049.JPG\",\n",
        "    \"2018_MN33_009632.JPG\",\n",
        "    \"2018_NB26_000679.JPG\",\n",
        "    \"2018_MT27_005639.JPG\",\n",
        "    \"2018_NB05_002216.JPG\",\n",
        "    \"2018_NB47_006890.JPG\",\n",
        "]\n",
        "\n",
        "md_output = json.loads(Path(output_file_path).read_text())[\"images\"]\n",
        "\n",
        "total_animals = 0\n",
        "empty_images = 0\n",
        "total_images = len(selected_images)\n",
        "\n",
        "for file_info in md_output:\n",
        "    if not Path(file_info[\"file\"]).name in selected_images:\n",
        "        continue\n",
        "\n",
        "    detections = len(file_info[\"detections\"])\n",
        "    total_animals += detections\n",
        "\n",
        "    if detections == 0:\n",
        "        empty_images += 1\n",
        "\n",
        "non_empty_images = total_images - empty_images\n",
        "\n",
        "print(\n",
        "    f\"In total, MegaDetector found {total_animals} animals across \"\n",
        "    f\"{non_empty_images} images while {empty_images} out of the \"\n",
        "    f\"{total_images} images were tagged as empty.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559b546-db0f-49c4-923f-2d8f8efdff2b",
      "metadata": {
        "id": "7559b546-db0f-49c4-923f-2d8f8efdff2b"
      },
      "source": [
        "### Step 3: Visualise the outputs\n",
        "\n",
        "While the JSON file contains all the information, it's not easy to interpret on its own. Visualizing the model's detections is always a good idea to make inspecting the results much easier.\n",
        "\n",
        "First, run the cell below. It will create a new folder with copies of the original images, but with MegaDetector's detections (the bounding boxes, labels, and confidence scores) drawn on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tjhvN5FpsScX",
      "metadata": {
        "id": "tjhvN5FpsScX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%%bash\n",
        "source .mdvenv/bin/activate\n",
        "python -m megadetector.visualization.visualize_detector_output \\\n",
        "  --images_dir \"data/images/\" \\\n",
        "  \"data/results/md_detections.json\" \\\n",
        "  \"data/results/md_viz/\" \\\n",
        "  --output_image_width 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9563ffad-e743-45b6-b357-eac9b3974a09",
      "metadata": {
        "id": "9563ffad-e743-45b6-b357-eac9b3974a09"
      },
      "source": [
        "Now, run the next cell to display those annotated images in an interactive viewer right here in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rn2MMAh6E2kk",
      "metadata": {
        "id": "rn2MMAh6E2kk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Visualise MegaDetector Outputs\n",
        "from pathlib import Path\n",
        "\n",
        "from ct_notebook_utils import image_tabs\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "md_viz_dir = data_dir / \"results\" / \"md_viz\"\n",
        "\n",
        "selected_images = [\n",
        "    \"2018_NB01_001794.JPG\",\n",
        "    \"2018_NB40_002921.JPG\",\n",
        "    \"2018_MT22_020230.JPG\",\n",
        "    \"2018_OMC11_009862.JPG\",\n",
        "    \"2018_NB26_025049.JPG\",\n",
        "    \"2018_MN33_009632.JPG\",\n",
        "    \"2018_NB26_000679.JPG\",\n",
        "    \"2018_MT27_005639.JPG\",\n",
        "    \"2018_NB05_002216.JPG\",\n",
        "    \"2018_NB47_006890.JPG\",\n",
        "]\n",
        "\n",
        "image_tabs(\n",
        "    [\n",
        "        path\n",
        "        for path in md_viz_dir.glob(\"*.JPG\")\n",
        "        if path.name.replace(\"anno_\", \"\") in selected_images\n",
        "    ],\n",
        "    width=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bjl_z7hASjFN",
      "metadata": {
        "id": "Bjl_z7hASjFN"
      },
      "source": [
        "Use the viewer to look through the 10 images. Compare the AI's detections with the annotations you made earlier and think about the following questions:\n",
        "\n",
        "1. How did the AI do? Did MegaDetector find the same animals that you did?\n",
        "2. Did the AI miss anything? If you found an animal that MegaDetector missed, why do you think it failed?\n",
        "3. Did the AI find anything you missed? Sometimes the model can spot things that are easy for a human eye to overlook.\n",
        "4. What's a good confidence threshold? Look at the confidence scores on the boxes. Based on your own judgment, could you pick a threshold (e.g., 0.5) that successfully separates the real animals from false detections?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2dbd29-26f3-4e81-ac9f-0d4954f1e82e",
      "metadata": {
        "id": "7d2dbd29-26f3-4e81-ac9f-0d4954f1e82e"
      },
      "source": [
        "# Species Classification\n",
        "\n",
        "MegaDetector is a great first step for processing camera trap data, as it helps filter out all the empty images.\n",
        "But if you want to identify which species are present, MegaDetector can't help.\n",
        "The next step is typically to manually review all the non-empty images, or perhaps to build a custom species classifier.\n",
        "However, a new tool called [**SpeciesNet**](https://github.com/google/cameratrapai) offers another powerful, pre-trained solution, similar to MegaDetector but for species identification.\n",
        "\n",
        "Both models were trained on huge amounts of data.\n",
        "SpeciesNet, for example, was trained on around 65 million images from all over the globe (see [this paper](doi.org/10.1049/cvi2.12318) for more details).\n",
        "These images were collected, manually annotated by many different research teams, and then shared on platforms like Wildlife Insights.\n",
        "The result is a model that synthesises a vast amount of expert knowledge to identify animals in photos.\n",
        "Both models are also free and open-source, meaning you can inspect their code and use them however you like, as long as you attribute them correctly.\n",
        "\n",
        "SpeciesNet is different from MegaDetector because it doesn't locate where an animal is in a photo.\n",
        "Instead, it looks at the *whole image* and tries to identify the species shown.\n",
        "Its current version can recognise 2,000 different classes of animals.\n",
        "If it can't identify the exact species, it will attempt to name a higher taxonomic category (like genus or family).\n",
        "Like MegaDetector, SpeciesNet also outputs a confidence score for each classification.\n",
        "\n",
        "In most scenarios, the two models are best used together.\n",
        "The ideal workflow is to run MegaDetector first to find and locate all the animals, then crop the image around each detection, and finally, run SpeciesNet on those smaller, cropped images to identify the species."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e70c281-882c-4316-a642-405d4d8b958f",
      "metadata": {
        "id": "6e70c281-882c-4316-a642-405d4d8b958f"
      },
      "source": [
        "## **Exercise**: Using SpeciesNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7be4d3-efa8-4af7-a6b5-cfd1b83a21d9",
      "metadata": {
        "id": "0c7be4d3-efa8-4af7-a6b5-cfd1b83a21d9"
      },
      "source": [
        "### **Step 1**: Run the SpeciesNet command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SfxpYfKY1PZx",
      "metadata": {
        "id": "SfxpYfKY1PZx",
        "scrolled": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%%bash\n",
        "source .mdvenv/bin/activate\n",
        "python -m megadetector.detection.run_md_and_speciesnet \\\n",
        "    \"data/images/\" \\\n",
        "    \"data/results/speciesnet_predictions.json\" \\\n",
        "    --country KEN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552bcae5-ac7b-44bd-9573-5fd48c27cb56",
      "metadata": {
        "id": "552bcae5-ac7b-44bd-9573-5fd48c27cb56"
      },
      "source": [
        "### Step 3: Visualise the outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034d51b0-d535-4997-9ad7-1ac31e42f790",
      "metadata": {
        "cellView": "form",
        "id": "034d51b0-d535-4997-9ad7-1ac31e42f790"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%%bash\n",
        "source .mdvenv/bin/activate\n",
        "python -m megadetector.visualization.visualize_detector_output \\\n",
        "    --images_dir \"data/images/\" \\\n",
        "    \"data/results/speciesnet_predictions.json\" \\\n",
        "    \"data/results/speciesnet_viz\" \\\n",
        "    --output_image_width 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82429c-a25c-442b-bd0e-4b8d32e9d168",
      "metadata": {
        "cellView": "form",
        "id": "df82429c-a25c-442b-bd0e-4b8d32e9d168"
      },
      "outputs": [],
      "source": [
        "# @title Visualise SpeciesNet Outputs\n",
        "from pathlib import Path\n",
        "\n",
        "from ct_notebook_utils import image_tabs\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "speciesnet_viz_dir = data_dir / \"results\" / \"speciesnet_viz\"\n",
        "\n",
        "image_tabs([path for path in speciesnet_viz_dir.glob(\"*.JPG\")], show_max=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a79058-6956-4200-b813-f020b0c535b2",
      "metadata": {
        "id": "98a79058-6956-4200-b813-f020b0c535b2"
      },
      "source": [
        "## **Exercise**: Working with detection data\n",
        "\n",
        "The output from SpeciesNet is saved in a JSON file, very similar to the one MegaDetector produced.\n",
        "\n",
        "This format is useful because it's compatible with other tools.\n",
        "For example, you could load these results into a graphical interface like [TimeLapse](https://timelapse.ucalgary.ca/) to review them visually.\n",
        "For analysis, however, it's often easier to work with a simple table (like a CSV file).\n",
        "\n",
        "Let's convert the JSON output into a more familiar tabular format.\n",
        "The cell below extracts all the detection and classification information and saves it as a CSV file named `ai_outputs.csv`.\n",
        "Run the cell, then use the file browser on the left to find and open the new CSV file to see what the final data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f863f14-5e87-41cc-9e33-5ecc8962848e",
      "metadata": {
        "cellView": "form",
        "id": "1f863f14-5e87-41cc-9e33-5ecc8962848e"
      },
      "outputs": [],
      "source": [
        "# @title Turn Detections into Table\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import data_table\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "outputs_path = data_dir / \"results\" / \"speciesnet_predictions.json\"\n",
        "\n",
        "speciesnet_results = json.loads(outputs_path.read_text())\n",
        "\n",
        "classes = speciesnet_results[\"classification_categories\"]\n",
        "\n",
        "\n",
        "species_df = []\n",
        "for file_predictions in speciesnet_results[\"images\"]:\n",
        "    for detection in file_predictions[\"detections\"]:\n",
        "        if detection[\"category\"] != \"1\":\n",
        "            # Ignore detection if not an animal\n",
        "            continue\n",
        "\n",
        "        if \"classifications\" not in detection:\n",
        "            # Ignore detection if it was not classified\n",
        "            continue\n",
        "\n",
        "\n",
        "        minx, miny, width, height = detection[\"bbox\"]\n",
        "\n",
        "        class_num, class_score = detection[\"classifications\"][0]\n",
        "        class_name = classes[class_num]\n",
        "\n",
        "        if class_name == \"blank\":\n",
        "            # Ignore detection if it was classified as blank\n",
        "            continue\n",
        "\n",
        "        species_df.append(\n",
        "            {\n",
        "                \"filepath\": file_predictions[\"file\"],\n",
        "                \"detection_score\": detection[\"conf\"],\n",
        "                \"class\": class_name,\n",
        "                \"class_score\": class_score,\n",
        "                \"minx\": minx,\n",
        "                \"miny\": miny,\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "            }\n",
        "        )\n",
        "\n",
        "species_df = pd.DataFrame(species_df)\n",
        "species_df.to_csv(\"ai_outputs.csv\", index=False)\n",
        "print(\"CSV saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425415e2-0987-4aa9-92de-aea695dbf075",
      "metadata": {
        "id": "425415e2-0987-4aa9-92de-aea695dbf075"
      },
      "source": [
        "Download the `ai_outputs.csv` file from the `data/results` folder to your computer.\n",
        "You can do this by finding it in the file browser on the left, clicking the three dots, and selecting \"Download\".\n",
        "\n",
        "Once you have the file, use your preferred tool (like Excel, R, or Python) to answer the following questions:\n",
        "\n",
        "1. Which species (or class) was detected most frequently by the AI?\n",
        "2. Is there a noticeable difference in the average confidence scores between species? Why do you think that might be?\n",
        "\n",
        "**Note**: If you're comfortable with Python, feel free to add a new code cell below and do your analysis right here in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d03215-afee-45a3-bb26-8e5cd2d2f8dc",
      "metadata": {
        "id": "56d03215-afee-45a3-bb26-8e5cd2d2f8dc"
      },
      "source": [
        "# Evaluating AI Outputs\n",
        "\n",
        "After running the AI models, we have a neat table of species detections.\n",
        "But are they correct?\n",
        "As you've probably noticed from the examples, the model's outputs aren't always accurate.\n",
        "Even models trained on millions of images will make mistakes, especially if your data looks very different from their training data.\n",
        "\n",
        "It is **critical** to get a quantitative measure of how well a model performs on your specific data.\n",
        "Without this step, you risk basing your research on \"model hallucinations\" rather than real ecological patterns.\n",
        "A good starting point is to read the performance reports from the model's creators, like the [accompanying paper](https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.12318) for SpeciesNet.\n",
        "However, it is **strongly recommended** that you always perform your own independent evaluation on your own dataset.\n",
        "\n",
        "Evaluating a model means comparing its predictions to a set of correct or *\"ground truth\"* answers.\n",
        "To create dataset a dataset for testing the model you need to manually annotate a subset of your own images.\n",
        "These annotations represent what you want the model to output.\n",
        "By comparing the model's predictions to this ground truth, you can calculate a suite of performance metrics that measure how well the model is doing its job.\n",
        "This is why manual annotation remains a vital step in any AI-assisted study."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8d8691-318d-434c-8fe7-87281beb09d9",
      "metadata": {
        "id": "fa8d8691-318d-434c-8fe7-87281beb09d9"
      },
      "source": [
        "## **Exercise:** Designing an Evaluation Dataset\n",
        "\n",
        "How should you choose which images to manually annotate?\n",
        "The way you select your sample can significantly impact your understanding of the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a5663ed-1a24-43a2-9516-5b5ec7396ba4",
      "metadata": {
        "id": "9a5663ed-1a24-43a2-9516-5b5ec7396ba4"
      },
      "source": [
        "### **Question 1**: Comparing Two Common Sampling Strategies\n",
        "\n",
        "Consider the pros and cons of these two common methods for selecting images to annotate for your ground truth:\n",
        "\n",
        "* Method A: Random Image Sample\n",
        "\n",
        "    Select a completely random sample of 1,000-2,000 images from the entire dataset and manually annotate everything in them.\n",
        "    \n",
        "* Method B: Species-Stratified Sample\n",
        "\n",
        "    For each species you care about, select 20-30 random images that the AI has already labelled as that species, and then manually check if the AI was correct.\n",
        "\n",
        "Think about what each method would allow you to measure. For example:\n",
        "\n",
        "- Which method is better for evaluating how well the model filters out empty images?\n",
        "- What are the potential biases of Method B?\n",
        "- What are the risks of Method A?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f40411-caf8-4acf-a5d8-660528ed3b9d",
      "metadata": {
        "id": "24f40411-caf8-4acf-a5d8-660528ed3b9d"
      },
      "source": [
        "### **Question 2**: What Else Needs Consideration?\n",
        "\n",
        "A random sample, while seemingly unbiased, can easily miss or underrepresent important conditions in your data.\n",
        "For example, even if 30% of your images were taken at night, a small random sample might happen to include very few of them.\n",
        "If the model performs poorly in the dark, your evaluation would fail to represent this weakness, giving you a misleadingly optimistic view of its overall performance.\n",
        "\n",
        "Beyond just day vs. night, what other factors should you consider to ensure your evaluation set is representative and tests the model under different conditions?\n",
        "List a few examples of how you might stratify your sample to get a more complete picture of the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4aab376-6e30-4e85-a836-e7365bc6507e",
      "metadata": {
        "id": "f4aab376-6e30-4e85-a836-e7365bc6507e"
      },
      "source": [
        "## **Exercise**: Measuring Model Performance\n",
        "\n",
        "Now, let's compare the AI's predictions to a \"ground truth\" dataset of annotations already prepared by the UCL team.\n",
        "\n",
        "There are many ways to measure performance, and the \"best\" method depends on your research question.\n",
        "The AI pipeline above gives us a lot of detail: it detects multiple animals, provides their exact locations with bounding boxes, and identifies their species.\n",
        "While this level of detail is useful for some studies, many ecological questions only require knowing that a certain species was present at a site.\n",
        "For analyses like occupancy modeling, it doesn't matter how many individuals were in the photo or exactly where they were.\n",
        "\n",
        "For this exercise, let's assume that's our goal: we just want to know if the AI can correctly tell us which species were present in each image, matching the answers provided by human experts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4fb4b35-1ac7-47fd-857d-923245823156",
      "metadata": {
        "id": "e4fb4b35-1ac7-47fd-857d-923245823156"
      },
      "source": [
        "### Step 1: Mapping species labels\n",
        "\n",
        "An important and surprisingly tricky step is to make sure we are comparing apples to apples.\n",
        "This means ensuring that the species labels from our ground truth match the labels used by the AI model.\n",
        "\n",
        "Because the manual annotations and the SpeciesNet model were created independently, their labels don't perfectly align.\n",
        "Run the cell below to see a comparison of the two sets of labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d2ded6",
      "metadata": {
        "id": "f7d2ded6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Label Comparison\n",
        "from itertools import zip_longest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "annotations = pd.read_csv(\"data/metadata/labels.csv\")\n",
        "\n",
        "ann_labels = annotations.label.sort_values().unique()\n",
        "pred_labels = species_df[\"class\"].sort_values().unique()\n",
        "\n",
        "print(f\"{'Annotation Labels':^20} | {'SpeciesNet Labels':^20} \")\n",
        "print(f\"{'-'*20} | {'-'*20} \")\n",
        "for ann_lab, pred_lab in zip_longest(ann_labels, pred_labels, fillvalue=\"\"):\n",
        "    print(f\"{ann_lab:^20} | {pred_lab:^20}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9359e668-e24d-4077-89b1-6b1a1f967edd",
      "metadata": {
        "id": "9359e668-e24d-4077-89b1-6b1a1f967edd"
      },
      "source": [
        "Take a look at the two lists and think about the following questions:\n",
        "\n",
        "* How would you match these two different sets of labels to allow for a fair comparison?\n",
        "* If you were starting a new camera trap project today, how could you plan your annotation process to avoid this problem from the start? is it always possible?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4746e310-29c6-4d89-94a1-a9f94b03f98c",
      "metadata": {
        "id": "4746e310-29c6-4d89-94a1-a9f94b03f98c"
      },
      "source": [
        "### Step 2: Understanding Different Types of Errors\n",
        "\n",
        "Let's focus our analysis on just one species: the impala. For this exercise, we'll keep it simple and only count a prediction as correct if SpeciesNet gives the specific \"impala\" label, ignoring higher-level classifications like \"bovidae family\".\n",
        "\n",
        "Our AI pipeline gives us two confidence scores for each detection, but for now, we'll just focus on the final species classification score from SpeciesNet.\n",
        "\n",
        "Run the cell below. It will go through each image in our ground truth set and calculate two things:\n",
        "1. Does the image actually contain an impala (based on the manual labels)?\n",
        "2. What was the AI's highest confidence score for \"impala\" in that image? (This will be 0 if the AI found no impalas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc8fab6-a5fd-4ebe-b135-cd30a444e061",
      "metadata": {
        "cellView": "form",
        "id": "6dc8fab6-a5fd-4ebe-b135-cd30a444e061"
      },
      "outputs": [],
      "source": [
        "# @title Get Per Image Info\n",
        "gt_classes = [\"impala\"]\n",
        "pred_classes = [\"impala\"]\n",
        "\n",
        "\n",
        "def get_pred_score(group):\n",
        "    preds = group[group[\"class\"].isin(pred_classes)]\n",
        "\n",
        "    if len(preds) == 0:\n",
        "        return 0\n",
        "\n",
        "    return preds[\"class_score\"].max()\n",
        "\n",
        "\n",
        "comparison = (\n",
        "    annotations.groupby(\"filename\")\n",
        "    .label.apply(lambda group: (group.isin(gt_classes)).any())\n",
        "    .rename(\"ground_truth\")\n",
        "    .to_frame()\n",
        "    .join(\n",
        "        species_df.groupby(\"filepath\")\n",
        "        .apply(get_pred_score, include_groups=False)\n",
        "        .rename(\"score\")\n",
        "    )\n",
        "    .fillna(0)\n",
        "    .reset_index()\n",
        ")\n",
        "comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27609d7-405b-4292-93b7-ca494939a8f2",
      "metadata": {
        "id": "e27609d7-405b-4292-93b7-ca494939a8f2"
      },
      "source": [
        "Now we can use a confidence threshold to make a decision.\n",
        "If an image's score is above the threshold, we'll say the AI predicted \"impala\". If the score is below, we'll say it did not.\n",
        "\n",
        "By comparing the AI's prediction to the ground truth for each image, we get one of four possible outcomes:\n",
        "\n",
        "* True Positive (TP): The image has an impala, and the AI correctly predicted it.\n",
        "* False Negative (FN): The image has an impala, but the AI missed it.\n",
        "* False Positive (FP): The image does not have an impala, but the AI said it did.\n",
        "* True Negative (TN): The image does not have an impala, and the AI correctly said it didn't.\n",
        "\n",
        "Using these four outcomes, we can calculate two standard performance metrics:\n",
        "\n",
        "* **Precision**: Of all the times the AI predicted \"impala\", what fraction was it correct?\n",
        "\n",
        "  $$ Precision = \\frac{TP}{TP + FP} $$\n",
        "\n",
        "* **Recall**: Of all the images that truly contained an impala, what fraction did the AI find?\n",
        "\n",
        "  $$ Precision = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "The values of Precision and Recall depend entirely on the confidence threshold you choose. Play with the slider in the next cell to see how they change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4320ef7-db23-4a2b-95a9-f8ddcbf50295",
      "metadata": {
        "cellView": "form",
        "id": "f4320ef7-db23-4a2b-95a9-f8ddcbf50295"
      },
      "outputs": [],
      "source": [
        "# @title Compute Precision and Recall\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "@widgets.interact(threshold=(0, 0.99, 0.01))\n",
        "def compute_precision_recall(threshold=0.8):\n",
        "    gt = comparison[\"ground_truth\"]\n",
        "    preds = comparison[\"score\"] >= threshold\n",
        "    tp = (preds & gt).sum()\n",
        "    fp = (preds & ~gt).sum()\n",
        "    fn = (~preds & gt).sum()\n",
        "\n",
        "    if tp + fp == 0:\n",
        "        precision = 1\n",
        "    else:\n",
        "        precision = tp / (tp + fp)\n",
        "\n",
        "    if tp + fn == 0:\n",
        "        recall = 1\n",
        "    else:\n",
        "        recall = tp / (tp + fn)\n",
        "\n",
        "    print(f\"{precision=:.1%} {recall=:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89cb653-cc27-40b1-a6f6-d18fdebabab0",
      "metadata": {
        "id": "e89cb653-cc27-40b1-a6f6-d18fdebabab0"
      },
      "source": [
        "### Step 3: Final Reflections\n",
        "\n",
        "Reflect on these final questions:\n",
        "\n",
        "* Think back to your own manual annotation exercise. How do you think your performance at identifying impalas would compare to the AI's?\n",
        "* We've been assuming that the human annotations are 100% correct. Is that a safe assumption in a real-world project? What factors could affect the reliability of human annotators?\n",
        "* We focused on evaluating species classification. How would you design an evaluation for the first stepâ€”the detection of animals (did MegaDetector find the animal and draw an accurate box)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc7e32b-2812-40bf-88a0-db86d4a36784",
      "metadata": {
        "id": "8dc7e32b-2812-40bf-88a0-db86d4a36784"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Hopefully, this notebook has given you a good introduction to how AI can be used for large-scale ecological research.\n",
        "Tools like MegaDetector and SpeciesNet can be incredibly helpful, and new models are constantly being developed.\n",
        "However, the most important takeaway is that AI is just one part of the process.\n",
        "Understanding your data, the ecological context, and critically validating the model's outputs are fundamental steps.\n",
        "AI models are powerful tools, but only when used carefully and responsibly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be2362e-4a76-4318-b30e-71679a1649c4",
      "metadata": {
        "id": "4be2362e-4a76-4318-b30e-71679a1649c4"
      },
      "source": [
        "## **Exercise**: Final Thoughts & Discussion\n",
        "\n",
        "Consider the following questions:\n",
        "\n",
        "1. What are the limits of this workflow? When would the MegaDetector + SpeciesNet pipeline be a great solution, and for what kinds of research questions would it be unsuitable?\n",
        "2. How could this apply to your research? Could a similar AI workflow be useful in your specific field of study? If you don't use camera traps, are you aware of similar AI tools for your type of data (e.g., for audio, satellite imagery, or genetic data)?\n",
        "3. What are the potential downsides? Beyond just getting incorrect predictions, what are some of the other risks or drawbacks of relying on AI for ecological research?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}